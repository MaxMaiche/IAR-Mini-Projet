{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ad0e1a3",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Prepare the environment\n",
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "\n",
    "    assert (\n",
    "        run([\"pip\", \"install\", \"easypip\"]).returncode == 0\n",
    "    ), \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils\").setup(maze_mdp=True)\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import bbrl_gymnasium  # noqa: F401\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent\n",
    "from bbrl_utils.algorithms import EpochBasedAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer, soft_update_params\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from bbrl.visu.plot_policies import plot_policy\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1732edc6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ContinuousQAgent(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        self.is_q_function = True\n",
    "        self.model = build_mlp(\n",
    "            [state_dim + action_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        # Get the current state $s_t$ and the chosen action $a_t$\n",
    "        obs = self.get((\"env/env_obs\", t))  # shape B x D_{obs}\n",
    "        action = self.get((\"action\", t))  # shape B x D_{action}\n",
    "\n",
    "        # Compute the Q-value(s_t, a_t)\n",
    "        obs_act = torch.cat((obs, action), dim=1)  # shape B x (D_{obs} + D_{action})\n",
    "        # Get the q-value (and remove the last dimension since it is a scalar)\n",
    "        q_value = self.model(obs_act).squeeze(-1)\n",
    "        self.set((f\"{self.prefix}q_value\", t), q_value)\n",
    "\n",
    "    def predict_value(self, obs, action):\n",
    "        obs_act = torch.cat((obs, action), dim=0)\n",
    "        q_value = self.model(obs_act)\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97fe4ed3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ContinuousDeterministicActor(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        layers = [state_dim] + list(hidden_layers) + [action_dim]\n",
    "        self.model = build_mlp(\n",
    "            layers, activation=nn.ReLU(), output_activation=nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = self.model(obs)\n",
    "        self.set((\"action\", t), action)\n",
    "\n",
    "    def predict_action(self, obs, stochastic):\n",
    "        assert (\n",
    "            not stochastic\n",
    "        ), \"ContinuousDeterministicActor cannot provide stochastic predictions\"\n",
    "        return self.model(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49d88bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b1ada13",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class AddGaussianNoise(Agent):\n",
    "    def __init__(self, sigma):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        act = self.get((\"action\", t))\n",
    "        dist = Normal(act, self.sigma)\n",
    "        action = dist.sample()\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "249794ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddOUNoise(Agent):\n",
    "    \"\"\"\n",
    "    Ornstein Uhlenbeck process noise for actions as suggested by DDPG paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, std_dev, theta=0.15, dt=1e-2):\n",
    "        self.theta = theta\n",
    "        self.std_dev = std_dev\n",
    "        self.dt = dt\n",
    "        self.x_prev = 0\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        act = self.get((\"action\", t))\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (act - self.x_prev) * self.dt\n",
    "            + self.std_dev * math.sqrt(self.dt) * torch.randn(act.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        self.set((\"action\", t), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b2da720",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def compute_actor_loss(q_values):\n",
    "    \"\"\"Returns the actor loss\n",
    "\n",
    "    :param q_values: The q-values (shape 2xB)\n",
    "    :return: A scalar (the loss)\n",
    "    \"\"\"\n",
    "    return -q_values[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "943cade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_critic_loss_td3(cfg, reward, must_bootstrap, q_values, target_q_values_1, target_q_values_2):\n",
    "    \"\"\"\n",
    "    Compute the TD3 critic loss from a sample of transitions\n",
    "    \"\"\"\n",
    "    target = reward[1] + cfg.algorithm.discount_factor * torch.min(target_q_values_1[1], target_q_values_2[1]) * must_bootstrap[1].int()\n",
    "    mse = nn.MSELoss()\n",
    "    critic_loss = mse(q_values[0], target)\n",
    "    \n",
    "    return critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a96a6a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(EpochBasedAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        # Define the agents and optimizers for TD3\n",
    "\n",
    "        # we create the critic and the actor, but also an exploration agent to\n",
    "        # add noise and a target critic. The version below does not use a target\n",
    "        # actor as it proved hard to tune, but such a target actor is used in\n",
    "        # the original paper.\n",
    "\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "        self.critic_1 = ContinuousQAgent(\n",
    "            obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n",
    "        ).with_prefix(\"critic_1/\")\n",
    "        \n",
    "        self.critic_2 = ContinuousQAgent(\n",
    "            obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n",
    "        ).with_prefix(\"critic_2/\")\n",
    "        \n",
    "        self.target_critic_1 = copy.deepcopy(self.critic_1).with_prefix(\"target-critic_1/\")\n",
    "        self.target_critic_2 = copy.deepcopy(self.critic_2).with_prefix(\"target-critic_2/\")\n",
    "\n",
    "    \n",
    "        self.actor = ContinuousDeterministicActor(\n",
    "            obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
    "        )\n",
    "\n",
    "        # As an alternative, you can use `AddOUNoise`\n",
    "        noise_agent = AddGaussianNoise(cfg.algorithm.action_noise)\n",
    "\n",
    "        self.train_policy = Agents(self.actor, noise_agent)\n",
    "        self.eval_policy = self.actor\n",
    "\n",
    "        # Define agents over time\n",
    "        self.t_actor = TemporalAgent(self.actor)\n",
    "        self.t_critic_1 = TemporalAgent(self.critic_1)\n",
    "        self.t_critic_2 = TemporalAgent(self.critic_2)\n",
    "        self.t_target_critic_1 = TemporalAgent(self.target_critic_1)\n",
    "        self.t_target_critic_2 = TemporalAgent(self.target_critic_2)\n",
    "\n",
    "        # Configure the optimizer\n",
    "        self.actor_optimizer = setup_optimizer(cfg.actor_optimizer, self.actor)\n",
    "        self.critic_optimizer_1 = setup_optimizer(cfg.critic_optimizer, self.critic_1)\n",
    "        self.critic_optimizer_2 = setup_optimizer(cfg.critic_optimizer, self.critic_2)\n",
    "\n",
    "\n",
    "def run_td3(td3: TD3):\n",
    "    for rb in td3.iter_replay_buffers():\n",
    "        rb_workspace = rb.get_shuffled(td3.cfg.algorithm.batch_size)\n",
    "\n",
    "        terminated, reward = rb_workspace[\"env/terminated\", \"env/reward\"]\n",
    "\n",
    "        # Determines whether values of the critic should be propagated\n",
    "        # True if the episode reached a time limit or if the task was not done\n",
    "        # See https://github.com/osigaud/bbrl/blob/master/docs/time_limits.md\n",
    "        must_bootstrap = ~terminated\n",
    "\n",
    "        # Random chooser\n",
    "        do1 = False\n",
    "        do2 = False\n",
    "        rdm = torch.randint(0, 3, (1,))\n",
    "        if rdm == 0:\n",
    "            do1 = True\n",
    "            do2 = True\n",
    "        elif rdm == 1:\n",
    "            do1 = True\n",
    "        elif rdm == 2:\n",
    "            do2 = True\n",
    "        \n",
    "        # Critic update\n",
    "        # compute q_values: at t, we have Q(s,a) from the (s,a) in the RB\n",
    "        \n",
    "        td3.t_critic_1(rb_workspace, t=0, n_steps=1)\n",
    "        td3.t_critic_2(rb_workspace, t=0, n_steps=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # replace the action at t+1 in the RB with \\pi(s_{t+1}), to compute\n",
    "            # Q(s_{t+1}, \\pi(s_{t+1}) below\n",
    "            td3.t_actor(rb_workspace, t=1, n_steps=1)\n",
    "            # compute q_values: at t+1 we have Q(s_{t+1}, \\pi(s_{t+1})\n",
    "            td3.t_target_critic_1(rb_workspace, t=1, n_steps=1)\n",
    "            td3.t_target_critic_2(rb_workspace, t=1, n_steps=1)\n",
    "\n",
    "        # finally q_values contains the above collection at t=0 and t=1\n",
    "        q_values_1, post_q_values_1 = rb_workspace[\n",
    "            \"critic_1/q_value\", \"target-critic_1/q_value\"\n",
    "        ]\n",
    "        \n",
    "        q_values_2, post_q_values_2 = rb_workspace[\n",
    "            \"critic_2/q_value\", \"target-critic_2/q_value\"\n",
    "        ]\n",
    "\n",
    "        # Compute critic loss\n",
    "        if do1:\n",
    "            critic_loss_1 = compute_critic_loss_td3(\n",
    "                td3.cfg, reward, must_bootstrap, q_values_1, post_q_values_1, post_q_values_2\n",
    "            )\n",
    "        if do2:\n",
    "            critic_loss_2 = compute_critic_loss_td3(\n",
    "                td3.cfg, reward, must_bootstrap, q_values_2, post_q_values_2, post_q_values_1\n",
    "            )\n",
    "        if do1:\n",
    "            td3.logger.add_log(\"critic_loss_1\", critic_loss_1, td3.nb_steps)\n",
    "        if do2:    \n",
    "            td3.logger.add_log(\"critic_loss_2\", critic_loss_2, td3.nb_steps)\n",
    "        \n",
    "        \n",
    "        if do1:\n",
    "            td3.critic_optimizer_1.zero_grad()\n",
    "            critic_loss_1.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                td3.critic_1.parameters(), td3.cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            td3.critic_optimizer_1.step()\n",
    "        if do2:\n",
    "            td3.critic_optimizer_2.zero_grad()\n",
    "            critic_loss_2.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                td3.critic_2.parameters(), td3.cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            td3.critic_optimizer_2.step()\n",
    "\n",
    "        # Actor update\n",
    "\n",
    "        # Now we determine the actions the current policy would take in the states from the RB\n",
    "        td3.t_actor(rb_workspace, t=0, n_steps=1)\n",
    "\n",
    "        if do1: \n",
    "            # We determine the Q values resulting from actions of the current policy\n",
    "            td3.t_critic_1(rb_workspace, t=0, n_steps=1)\n",
    "        if do2:\n",
    "            td3.t_critic_2(rb_workspace, t=0, n_steps=1)\n",
    "                \n",
    "        \n",
    "\n",
    "        # and we back-propagate the corresponding loss to maximize the Q values\n",
    "        q_values = rb_workspace[\"critic_1/q_value\"]\n",
    "        actor_loss = compute_actor_loss(q_values)\n",
    "\n",
    "        td3.logger.add_log(\"actor_loss\", actor_loss, td3.nb_steps)\n",
    "\n",
    "        # if -25 < actor_loss < 0 and nb_steps > 2e5:\n",
    "        td3.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            td3.actor.parameters(), td3.cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        td3.actor_optimizer.step()\n",
    "\n",
    "        # Soft update of target q function\n",
    "        soft_update_params(\n",
    "            td3.critic_1, td3.target_critic_1, td3.cfg.algorithm.tau_target\n",
    "        )\n",
    "\n",
    "        if td3.evaluate():\n",
    "            if td3.cfg.plot_agents:\n",
    "                plot_policy(\n",
    "                    td3.actor,\n",
    "                    td3.eval_env,\n",
    "                    td3.best_reward,\n",
    "                    str(td3.base_dir / \"plots\"),\n",
    "                    td3.cfg.gym_env.env_name,\n",
    "                    stochastic=False,\n",
    "                )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81372d39",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c63adc2ab3a4ded8ae95f4fab13a539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video of best agent recorded in outputs\\LunarLanderContinuous-v2\\td3-S1_20241004-120939\\best_agent.mp4\n",
      "Moviepy - Building video C:\\Users\\Max\\Desktop\\CodeM2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S1_20241004-120939\\best_agent.mp4.\n",
      "Moviepy - Writing video C:\\Users\\Max\\Desktop\\CodeM2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S1_20241004-120939\\best_agent.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\Max\\Desktop\\CodeM2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S1_20241004-120939\\best_agent.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div align=middle><video src='data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAK7JtZGF0AAACoQYF//+d3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE1OSAtIEguMjY0L01QRUctNCBBVkMgY29kZWMgLSBDb3B5bGVmdCAyMDAzLTIwMTkgLSBodHRwOi8vd3d3LnZpZGVvbGFuLm9yZy94MjY0Lmh0bWwgLSBvcHRpb25zOiBjYWJhYz0xIHJlZj0zIGRlYmxvY2s9MTowOjAgYW5hbHlzZT0weDM6MHgxMTMgbWU9aGV4IHN1Ym1lPTcgcHN5PTEgcHN5X3JkPTEuMDA6MC4wMCBtaXhlZF9yZWY9MSBtZV9yYW5nZT0xNiBjaHJvbWFfbWU9MSB0cmVsbGlzPTEgOHg4ZGN0PTEgY3FtPTAgZGVhZHpvbmU9MjEsMTEgZmFzdF9wc2tpcD0xIGNocm9tYV9xcF9vZmZzZXQ9LTIgdGhyZWFkcz0xMiBsb29rYWhlYWRfdGhyZWFkcz0yIHNsaWNlZF90aHJlYWRzPTAgbnI9MCBkZWNpbWF0ZT0xIGludGVybGFjZWQ9MCBibHVyYXlfY29tcGF0PTAgY29uc3RyYWluZWRfaW50cmE9MCBiZnJhbWVzPTMgYl9weXJhbWlkPTIgYl9hZGFwdD0xIGJfYmlhcz0wIGRpcmVjdD0xIHdlaWdodGI9MSBvcGVuX2dvcD0wIHdlaWdodHA9MiBrZXlpbnQ9MjUwIGtleWludF9taW49MjUgc2NlbmVjdXQ9NDAgaW50cmFfcmVmcmVzaD0wIHJjX2xvb2thaGVhZD00MCByYz1jcmYgbWJ0cmVlPTEgY3JmPTIzLjAgcWNvbXA9MC42MCBxcG1pbj0wIHFwbWF4PTY5IHFwc3RlcD00IGlwX3JhdGlvPTEuNDAgYXE9MToxLjAwAIAAAAatZYiEAC///vau/MsrdH6VLh1Ze7NR8uhJcv2IMH2U14eWG6wBE5sll1xIiijy1FS38g1QhzuI7g8sFJ5On3gWJ//X507i/v7lVFrIashLz53/FofoPlywc0LubPM8pdjcuxpApSnywTT2GJgwqS56LJQaCAAPuWpQvYmKuuOH7fxdi8GvoTvSJj5NdMjSSAdBDQwbwgX0fvJmW4450gKtJLTT0nkKvR6FxHsClhNlf/0fS/eusU+Rujo+x4iOf/FoBZ3Kn5EK+IrxYFOAAAADAABjg9hC2JQjeALQtOlmqoP4DI6Qg5iHJzUS3CeNU8A1xE517kQp3RT6kKEgLogxhNDiea6TQAANcD8tZAR0RUSQYgmhByRkSLAb4AAVbHsdjx0ytRiH15A0gaPwCHbh8GCnALWmfdoWQu9TA6Plkm0OAMygd5T91g6Mmr7i0kyMTfxuwC7WdhaXoD4xzhj0s2+BXsRfS9v2GQKBxevcKLRIQNg3paSkB68tJibGNk+0N84N9Hv3eNrqQl2Sr8OfNHjcZ4XudgCLF6eBxyATYN6TOoCh/qYbJuqqqfEuAj8k8vpqJaHwdV/eSEtWVPDcREu0/FYdYfIwFgA6NfVkSwk4t9NVx6hI2ydVY21x2Cofuf0EH6SWuG1h7cifukG96M/s8SIKz0I8hnah4G8OQTUNX2jAq4ptbdnNFG81Z2LrJFlmOwv9EgXZn9plRrWaqeVpfg9buannt6GT/8mAEVf3huUSWT974RomQL5JMVUQgPH5/wu0ZQrkzpLNpAMRfkQxkJSuYrK8f2c67M2tWiNxTg92MNiaesl5ADjFN1HdBoPXGP4UsfiinVEZRskK5Q+yp8pbd0nA5yVs9+z4sVCeJ6Qp0hvgnb45MU4gDFu5ug+N2cxIXjg34xngvEcK2gARQgRZWgBoc/bT+u6e8KF1DHpAIW3CuOCEzkJm/HKO1kJgDJ7NdFP/gp6i5AfD6OjqE73kgEW9NnBgl0cmfKuJMRWYnuJoT/3xIoxKx6EvD04bYnT7mxFYAjelsjvBuTy2RNWB32gD+oh830/W41wkpO+OwKX275qOFpuAaXc6fkr/D7lmkI5wQBhF/hPsvO4yWH7r+4X++7elLJsOL/FSOK1ZCVdQXIcXiKjcD3+Ki6keHnQm6ayUfucJFcITF4apNT1NqwSsvUIj1quBZpadfemlOxOj3aSKQgIxggmWt3vxVVJSivwlNF8BUHcFRi1ts4wkSEDck3ekM9/pfVbZlRbNjndYLQUgH+ws+jCgTfjRPOMzJHKrWrUbOCdzSoxf/Gj8eppT1FtXih0KS0pxI58H5sBlwOGmkD9zYk9+m2/T6lFS794eqlvOvEPnFq2NjeB3eA4IfjZaJ0cWoNcvcDjbR6RpsBQlGqHQkOTdWfiKBE/KQSpSPw03deC0OgUMSU06r5ow/pLDXdZFwe3WIq3BZCI/rxQjUoUIo7WrPq4GVf9q271CRhA8l0BXD6x0GElYSAdEVXaebSSy0wxB4lZGgmuObU59VtTOq9CGRM1SlJd5aMpBMIjlj0XprUgAAbIQKTz1ybf1ve7PJ9z05iOvY6jxhVaNcFBqutsM2cb3ORCNn7kIXkRrjbS0elHmmqivNAlzLk4LJSTCiO95/B3Pjk77OJ8ZMiww5QpCPA8g7BLlpg5pHUICyDwq6B9sJmdLrhuBCm7WuAaCM1M4dmSQrjFJWcPJFIBZnDL7sTQdHx9mfLOML5uZEcTAqNw+Cw5fipe5ScKap02ebEf1/dr8dAB9Icp5SCUrry+xRbzw9kgop9x0ABx+5e1Hk/Da05qbRlYORJPshvhqfByFMeyFVAJ7eiqzJ2Xbn7TYa96FM/17Bz5z876A+Cb67X6b9upvPPhL8Vn4zvdf0Uwuxf8+buGdIHRUapv4zPtnyl2ZcAM5+KAK7XqhhtVC6hRUXqssy66b7GmxqnXIsVi7474l8IpbycjI33RV48QFSUHDwDvoIzsUheBTCj857RtZq4N6ZxXdRFOHxZP9wRePxytV4HaEyalECkzrfKD1fGKsSOr5enuyvyCczrAwYzQVTFbgZVmzrD7nnYa3Be4GK8z+VMPFArx6UVklT8mf6J+pifcp888J1CWAm3RoNeyUYuvoYAArboJidrvn4Tx713jGDR5RDnylx88cnGGEn+/tBED0J39fmRTVGMyOYYjul5CAKJFe3i+TUFXpsYGCnmEC2Rz7FM8+kO6aScxe1nH8ul8EKN8IGnbhOJWLpqAN6AAAAwAAAwAAUMEAAAGaQZokbEL//ozPzpGXwAcGjuS0TcuXCurbha5miPNoPvpbxhgSHO9X7iQD+yFIYd92iKurTfaKPbDyERuEOKONcr/PRP9Yj1nI6xkBdZSp2SX03RkHSMAZ6ulkx2R+A+g2a60t3jerRs1PgHx7dd5u2bfxKmxXAAADAAHGnSrEMZ1Ue0/O+Ux6nwOuwSUlyYWyjtDfeiG+RydY3iY8sI+/8sAC12lwCUMSSrlX+lgpNb7sUXb2qAZQ/U5/1y9jPJ1kuXvdtUzVhhoUDeEj48Lyc7CLtax3T2puYcttR5MjgInSMf12KZL+gH6/HqVJbyVweGG+rkCbs1Jg7UbZSGPSmvZIPnGt85OOx1DQC62cbPwGu1s+H1bIRoA/Fjd523XsOfplkmpZ09Oq9IWJev3RibvdqkmO904y+eSvwxmX20JtxrLDelFhMw3AIQL5oc9UE2SOk36LNbLLa0DtI/e7T1Xx3y0sIVj35MCIz/asSlROIBk5RPeKEoKhWcGmo/UbX/K9ZeIY9QTCAc9lj/YYOPQvaFsLQkIWcOAAAABpQZ5CeIR/D0/UzHreXNJ8/uW3FA3IHACo2Iy3nXiPH7WfWExFCQFG8fW/1ObgVOkPfJCTRbGEMwgJ2Pe07tWrYAM4BuXt8zMT33G11rBUOQB0ipX5YzdmDMTOj/IVehdIEVqGqYZgYB/hAAAAOgGeYXRH/xTrTjKbTAnwLwAOJ3yZsY2RGES19RhxkMpK+tS49wAE/lhkwBBilyw1sSSSiw8wZWMAtoAAAABQAZ5jakf/FKACeYvHwjrbp/Lq+ySx0TI7WABdRMitW/F+KHt+LWuHzQ5D8cDljU9kAQXuZTcmKbumHPeEQAK+p7JjFgdhXKxpFvwYfCYAFVEAAAETQZpmSahBaJlMFPC//ozPipXae6NmiDJSmjJNzERkUALdqr4trDxQSt9atSvw6n/8LofwonyPfXKjrSBfUwNRKQ9GuDM6f/QmkFO3QAAAAwAdMOxUcdVdj/01a8gCJZbHqOnEEUbuiZ6mUwl0/Y7FDD25K6HH757+uBerNft2B2OyOLSWtCC4iHU3MBx0Jjgl4UtDq4UZjmaTXvu2qZ0/tf3RcjCdfEH6njZv8RHsJ78isz4jwrU4xjgBhntYy4NrQ54/2szs8ed2TOC/at+YkYNqMo35uv2HjCTdgMTauzqjUgCO5ewI1H1pZdaTbrvknTZPA1KCvKdEK2Ptw69Sf06A9OmQ/gn/R+dibNc/5kaAKCEAAABnAZ6Fakf/FJsxG3f0F3ryefn7DtyhGK7wkxJ0WdfYGP44AJwjmFsz7hLeWM8GeoWrJnu/Vp6lhfo0llpneIz+hGeT5hgDIvcTnVmVAmIf9BTtyUFD6cQsq6FKZtG/RH8891/5ZwDKgQAAARVBmolJ4QpSZTAhf/6M0zN/qVTq106bxJCADafqJR7mkFVPOoz/fcB1JoKGbWqRum8bZAB1yLi0fCRpQLVS5S+BtRJfap9oZhKI0bLJeOjVl20djMGXmDgAAAa5QYbUUe0KI1BJyLbwV1h/kRVoBJ1K/v7MCKA028ihCwKLgU/cj8XlLELyKmOmAGPvu2qZnO+8IkJ/oflvKcUrqBNXkeGaLZgsY9QzEOutoCIMwq172CNeVJshxUR3ou4JH6sVvdc1pHF69p9gaEwiDPtZYKOScU3RZ1oGffsfIHJRISno9/4wY+IxVpQLIqbtQ5pcNlrej8iMLnPeIs5NY0WRkw4PKDGa25FImiarALJD374lWOun+wmZAAAAcEGep0U0TCP/D1voGcrVgymQk0avZrQAFtSqYuVDw8rvmah6gi847ZRhFk9XzSJvgNBcrJXR/IrOIUM9vfLdC0G5qR8AYY1xKBa2dxT2BuXCYA2FiPPVBEra+jg7ngxMBCoUrNhs7NfQ5/PqM0/GQR8AAABIAZ7Iakf/EwP+FY4Q1Ju8bn6T3Gs4UVdnqewqGobbxG8pTUw0AWN3VvYj7x0AGiEUXVgiMA65JXDOcIWfC7XmLFPjFV3XHCggAAAA0UGay0moQWiZTBTwv/6Mzu+kzC/+lZb3x/yzhAXjGJ4AAAMAAAMD5UZY2QKwQAimk+AILFYnNFfiTX/oi7rwDjvuYlU+0EJb+MRZKoiKHTDPcctTCo4b77talYlyfo72cYXw+E1j9OnIi2pZYpGXxWliVp1dOXXJ6sgCcduV2HrI5UXUPX06/UELyz3B+k1cHVjdDgkFqtA/IeNYJNaitKwLESGfrDSuwUR0UdsEXK6PKEgmyrrafK7JX12szTqWR4ifDgpvPR2h+6eiFk8azBhRAAAAZwGe6mpH/xUpfNQHK4Lg9F4cACdOZ5djzaaz/qynAT7UEudGQv4zrwIWD+3N6khn8xPdNL1nRd3sndmgYoeLUFVnnAKsH0UJ9oQ7EDVVJvfUtvwZXay8scAAPXiWKCu8DQImb9vdVMAAAADiQZruSeEKUmUwIX/+jNMzf6vjzk9OgJihBaxWbKpl+gA4ypa+Edllu8pY2xa0rCA5iMV/d6gjG7/JLfIbUTyrmzlcCALNU7BvzbkNkudkTxysW0GKEbGxBhhfjsVtumlNldBKLvmnwaddLFtuqGgTEHV6SQuj1P6AT25IYkOdzEbeSYtxlVuPbI7MgN3ufojNk88bEuTCOTQ+cPu2Ipk6KHNFbrMC1cftmQvcEenWsdD6ewnWmA+bfXWHzcOTtYAA8kRwzQA932Rf0oN16c63mh20C5+DdPpPWuijiIjbSytVggAAAGVBnwxFNEwj/w97JvkSG+VAiYB2Fec9HLwAOYVkkZ6wi4pSe77Itsy3X+CcXZFPyHAcva6wmbk0xe7xdcFXvCQn5TXAcB2WMLhXAaRsnFJ60GAiTHIbE8rECqoSLIQ9FVZTkemHHQAAAE4Bny1qR/8U/ZRLSBND/Me77JNRSJ8aqrYMlbn8Cv29Rv4RYIxV5+pGQfvxzQAjpXhREVibadXLu1wwo1tdi6yX9E2RkWEIBPd0D86IIR8AAADDQZsySahBaJlMCF///ozGumFgIZniXW6IAL6aFE7Yv9OONwUji4JwFu9MT56/Eaw3wPxW2yFS234rRCEbKP76dr6wvCSD4Ipvb6LdKi/OgwUQ1J5L4k2ODJg2kLrhll0JmA3f5qgW9cfDdJxxAuyiqF8BzsLUWzE3eOYEHiN8Ewym5G+a5RfP31zoUks/G8Sd7cXI73RpgT1JohvaC4/ErMXwpJStxq574/W6rxkMiiBjVu2xiB+A9dItht0kPD5J0Yu5AAAAUEGfUEURLCP/DfDlaao6pl7cEIVIWtS1rlY1AjQmGjfQRUcRiqD4qIFV47AC3hbmaGgJuNBerNkW5NPg2XzOpTjamBt5fgZMiwYLLiMEucI+AAAATgGfb3RH/xMES4i5F/pFGfgv1l6gVDKPaL13o96iuOv7joRbK+JYxW4wO63DW+fE2QgB0AXn7iB6JQfCDCx3WtxeGfIY4U0gHxYHLQBxBwAAAEoBn3FqR/8Dp1Rn9r4gcmyz1EIv16xV5VQGEz8/NDjZe7cABKLZHsjOEhJUfhg2mLbxng7XLgWzEqEPNv85KzAK8918z7NHfR5JCwAAAKdBm3VJqEFsmUwIV//+OEAin1PI450+L2Fe1l0myEUMjwCuoZdEDlQ+GwXxmvvvzcO/9QV2VGjTtT+2+O4ZTbv3lN4f/rWImuQke5blPTPaXzfi3rKCmt+A/PhRrgTciwm2NzIic89PLDq8S0zp//n2/s72eBTZQWtqGncZv0mU7ZNG4Uz9GHV9udOoO6jpG/2Y6n9rRFj6oXsOswDzTzob6F16NPJpHwAAAFVBn5NFFSwj/wJa/rKfBkpVpkTMj0qiSarqz+6rlOJ03U2EAAyAD8OKZWAv7hV/Oa8z++zWJSHwD9k0PxDVcyTQfFIlOwe1hcSwHOSgkiIDmdgM0FTAAAAASQGftGpH/wOFm/Vp2GH5MMUNCJ7c9Cf1TUEQTocazsS/TKFSiqriSjT8lHrjHHJcaoAGfPu4oNWk6+A4OD04iuYH0JRzeWBAQtsAAACNQZu2SahBbJlMCF///oywAsnlpA9HvME4zf9sNXiACCsfhPxW2tfbdf/R24LaYC/zBuRJhsMjNvgpihzF8Zcg9YxTkcnrNSutUEmwJa4usDhOfJcrq33TXBBBURNoD7lJVxoi+SYx1UZMPg8oY1NAYrZ5Oo35WsIZKKZ06jTPMbvFz60N7o/lhLXDcLuAAAAAwEGb2UnhClJlMCF//oywAsnHeEbz6TF6/PMjMk9XJSftVLHhQ39sGur8Rl4wV1AbjWT2DAz8fffWGnE2c2+YbGhDYZGBbwLZ8gxYqGrAg++7wmDD2tSipS/anXnQzuMmysl5jEtQzUiHTqKB0M1VB8yceTyNCyNFFSIiS7sx7gcjQcgMIQle1JwciYF3BLFV1MB0oj9UF4mxDvVRbQHeX8B0DgyLehpXh9CTjP0Q5ppKfQu8+nET+Y48MM8A1wBnwQAAAFBBn/dFNEwj/wDivAQLEiTShKtdl1jIhl/ec5FKWAXt9fMe+HnxO4QgBL0SoDT2oykOmJ8IhAzR2WbT58VF2aBZ1m8GFQuFvVyF1a1o+CAGjQAAAFYBnhhqR/8Ba7qVRXPjJcIPrd5BAEIoxPkypbXOjQWIMAzpFxA0b4bjP1Z668Hfnt7wRGsaaTJKXPGbHpVAcsJLO4eJ2nOn+Gq1B3OJu5AKjlCM4UIj4AAAAJBBmhxJqEFomUwIX//+jLACu8y5jC5p1LWujwJTEPKFLdm6xr2vPQXD/yXaextoMAC83ACXbPzhD7aSoC1tnfshwS5rcf8JxOCkBdZGMBjQLbuvEffWdCf7Ze5hSmBn+oDnfKPLKAwXq80Kto5O/AbcPe+oU7oOdHjuHu2B4lIMcWrjrDqNm2qHEHpMEIq30YUAAABcQZ46RREsI/8A3RT9HhPDzHAAgK5d9qFTh2YJkALEVU74MicYi0VRO0dkghX3Z7jx6NojYotU6oRiS1NrxhrcRJa36/nWobSxigYNLlmZ4xlp+myA25gWruLiHzAAAABJAZ5bakf/AIq5/AFL1MNdDAuTjyiF4kLzyL8J5Y2IAFktl7Vg4mVjL9GuYi+Aedq+Io7h/VJshnjJSvuvsg/bxkeI1RDF5AJnwQAAAMtBml5JqEFsmUwUTC///oywARBWQcF7lWd9VScjCsqufnTl7smxw6aYtKuT8/vAOxQP+NxSMXgtvB7enbGEZRr2XR5iXJB5wClHWgU75ZHErsrC9sWtcpTbyM5/D4VPyBPqDD+vjsmqnmRulgaBnkONnULf4itsG5CmC3otI/ehISgNetVDkjxsG/NONJoF0wPbVjDMzFHFSs7mk8Z3to1bLqv7JHOA+LzfZTkvNPOIHtgEqF9/rniAKjuOqCyJ0fRdaoaklrAfZYMO6QAAAF0Bnn1qR/8AirWuPHyigFvtt5uSsNJpr4D5VT4L4LgAmXX2rXij9166CxezMd4QLx5qUGV+FaEyV2475kBhx/KUaty/cuWxsdvR3J9Gr9ugok0vQuUXmEJ1hSsdNlwAAACyQZphSeEKUmUwIX/+jLABEem9Nd8cv4juWqhkBwwlh017lKdbasUliCHZrxl58YARNnJ0rdNYAlc0f0cFLdUMRPraLSM9p/NnsA9T1F+sI2tveqNEIHAgThBQMGZDvNNP+ba01EwkI3XuyFLgD8b5IyEI2eLdcR1Azbg3ORSr9PMjJZF41gX64hescwtENrZf3b3wL+gMqmBlF6tAoSUx4D9mtijh7vmzMxhwjNCLn2HhZQAAAFtBnp9FNEwj/wBYlKsdpkjS2uhRQFsmVBCrxd4he8rxXIWkUddlOR00zKKqL63TzViSADjEW8tNGaabUpZCcAqQadp7Lc4enJ4wof+9LKB1zeOHDXuwpHctFggZAAAAXwGeoGpH/wA0un+QLawM6ACKSwNpx57WyP8bOsM8CZleTOBCLp+4ACILXg6/RK5QzvrF7ZAlHj4J5d9gwYjUaoQK6niZ13DTxgtdUCnBLiOBSJJRoem1OSwaAPVLrl3AAAAAzEGao0moQWiZTBTwv/6MsABnlZpI76Uai0XHFiAAFWyvnmPpvVmqijBz8wr8R3G6kjbbhSuatH3U9PTIaknVszXBnwfpPWCHwXUAR+cu6BoLfQ8ixpq7fcAAa1brFR5X2tQJRyeGkTp3qOr3+sd3hN2wRSVfA1spAiEYA2zA3FTSopUgMcAhODRJxnPpw7sgNRfqCqd1Lpx6slf9/EVSIeQ2fj9rR9pixuhpXVDISBkZBg1JJWWvNnWtiCa4sM0TWGidT9mcCr2VEAaBQQAAADsBnsJqR/8ANNBK0UeDY8iH/Q5yWEIdrtHTEYrNcmdOgl9JSpP1W90IAX+YhJ8+peA8qlbvI1A412Z8wAAAAKdBmsZJ4QpSZTAhf/6MsABoOE6a7574LctRcgX3dDMTH8QTuF9zQrWQn5hYAI4ssnHBfT0J+W7g39vj8iHloRSiWWm1sMGvzZ6fD3/S52ECy0Tq8OQ2m/3IfLHb0UwTQzs0+4nSqSmE9u9DMp50F1fThtzQS5Dqpfpp47aVPXp/W3uXS0KNe4sH90XA+qqn1Q/zISPZ1sYMA/PulwZ0Am9gP1FUvrWBFwAAAERBnuRFNEwj/wAgr/qAKh+FR5zf+xc6KTCNilLCLzFsD9vemiLktHiIfIAIHVqPpf5Q9YJTye2l0gw3d+JYh8RCr6AOmQAAAC0BnwVqR/8AM37abMeBM5KRb4O6yjQERg29R9jEQit5ufL60E/r8ihlsLlwgZUAAABwQZsISahBaJlMFPCv/jhAAJaCh6XtPQ9lcf3n6EBtPZIABBmY7UDEWXOSa0+c3ZGPbM3eBuyrOujnm54JCop4ieQ0Chx3Ca1C2Y5B5mfDWQiaR0yYq/Z5K+vly7mxHs3oXrwR9PBteQacLXuHJkXSXwAAADMBnydqR/8AFHvci3XWmzsAS3njRWCVX0fadlI334CqIgvWb4XFVoLjJGKZQYL19LFZJcQAAACJQZsqSeEKUmUwUsK//jhAAJd0TZeP2GTA7l2yoRQlSYO65l92r/801vsCX0ewBEln9VrgS3wCAgUvdArHJiph3zA5JYwCqMawd69TVMc9IGxzsrOaKbIzzKyxvaorgdX0X7kyrtBOQzeRyvSScBxFG327V0Ovn9ZkVU58GyukwapCllHJEtRnHzAAAABJAZ9Jakf/ABPkwFZjwSnJyxY5OuiDaB/kLYboq857/wwJtMPgBLIb0pU3kaeVIbgP9dBfm2+p6s4bS8RAhs3tJnGtuPg2HhjQcQAAAHRBm0xJ4Q6JlMFEwr/+OEAAOennt0Kp7vNgeQLuNBRnQ1qag9K0PUaAFtHu48MxLylnoEd/KJhI2vhNxGzr+7lQnYkCiBhGtb+H6E0o+PBbwaAzSLfRBn/ZJaOeqbWEGRgBoHlINTv0ZRxFI0d2posRmEp3QAAAADUBn2tqR/8AB5uPXOa8eOuuGpBG7yiVKowZ9340+yWgA/ZG0zC9zgoAjzAki3Wbxy9oYtFSPgAAAGdBm25J4Q8mUwU8K//+OEAAOjv/SNF6M8FkpcvDKPdQpLrv/S8cA0tbOMjlSTPmy3E4Hi7nTdbLqSymB1eQdEJDRjr1kBf7sXU3thyVZyoC7sg+pOJvqsGHkxOH9Mfu3GTZjIsWCSndAAAAPwGfjWpH/wAHlyUaUZDAySYmCSAp59AB+yWy574BFcta6fKfjhix1pEKMehkaH8ETr+ac6k6UFlq44PEQa4B6QAAAGlBm5BJ4Q8mUwU8J//98QAANj/TdG32W4QyLLEP5uhx89C/RYoCxoP8UxVcN6AbDEF/RQtT6pQK41nkuARktbxm8G0HGjOjBDNSVmg8z6Zycj7DglqoxDNNN/waB+ijJ6jefp5P82woFfEAAAA3AZ+vakf/AALoJSd7xIf+WJY9i4IMPGBcGyS4kV9QddcPi656h3EYJ5y+RcThKGIXq4LhxwAy4AAAAHVBm7FJ4Q8mUwIT//3xAAA2O/7WYxzCbC5YpNWzJsGYUX5dh8DADmPd7i8hhHEzsi+SooilHWClEgcgz4QS1ub+1f3NQfcq0yHpyJBXO8AjgaKa7rYNPgepk71DItnvil805zvXXcTSjf/h0ztu93W9Kw/o8f4AAACQQZvSSeEPJlMCE//98QAAFHWhLRBIVkYpDDQAtJA6pg0yOo8fTd6QIA6jqR+byHPC7C0veR3e1vazX3a7E+PGh0U/wVf0xBHTVvTXRySgBlrQWLrgQ1Ieqpw3GWT82DqwE7iP9hR4PKOozyIs2NBwafQMQtjeFxa09xG15T6EZZ57HhN4TkL78dDxT+sohR/hAAAAVEGb80nhDyZTAhP//fEAABRoAa0SCAStqe+pcpmkwb+w7C29zyJARAW+UnfvWxYvkkiBnO1V2htCnso0kT83TIdujVA+bZM9N5YKhEoLDtbEViZcQAAAAI5BmhVJ4Q8mUwURPCP//eEAAB/d/f2hMVBwGt4QkNUa69fSorAGq32bYANURHvS08G9VNhQg/0ieGqHw5VV7d06eHSyE+3oh+bNQhCRv5GEdLjAl3Ri4NwYWsP1U0BpgEGoYumrnmb9BmTxyPvP87zOdqURSoZPl8PjmqCwyTIoyhCfHbJXt/kjg7nC4fI+AAAALAGeNGpH/wABFY5hq84Xi6kueV8MSFpW/DG3k3UBrUm8dQf/WPR+7r/7e5MHAAAAfUGaN0nhDyZTBTx//IQAADE7/h4v5MIBbnDjJtXjrWZprP9HaCXOGptUftsz7qPHZDSqWlYNgJHADbv5aWLYXcpJDW71XRuigCF5q1ZHlXSOO/jcwJVuxdCg/10mhBIgquH641kuGgUR2BY53spJS1iQ7VPKnqCiJhEGwltAAAAAPQGeVmpH/wAAa/2F5dQTjoyhW0E34S0E3oKh/FjrDuCbTj58yPbavY4Tr29jaOvZrqOedvtxclvvV5bgBl0AAABdQZpYSeEPJlMCP//8hAAAEkxFewjqzAlwV0Q3QLAlcAJn3vFqbKdu4UrVhscpl2VTpNPGsR//sxpnI+yVo5r2OJwqyCSOEKr93nSh2+Hc7HO0Ks0C0/rN+tXrXRoxAAAAc0GaeUnhDyZTAhH//eEAAAS7/EAAlumWAXJenDVeENdT6NZOY5icFmqH7wvdrImowJBjURdjWUuydzLnfJA2/tx5QkS4vErEQTuJeqnz7fGqMa6Bx/D8IgBovvXOq+L1oDaDRB5uoXyzf7Pmk+88cjs4hzQAAABZQZqbSeEPJlMFETwj//3hAAAEm6JtKACISA/TzfmuNFokv0csKdMj6FxbR/rV+ACsnXn2r+7wdmMAZl7Mbraq7QKLodrwHfovYdCwoyz8nN15bQ0kxYpDQV8AAAAtAZ66akf/AAAnyYCsx4NS5PKp5uaSCP0AS1jhIUUTzfSL7UZ+3KDjPwFH3B/gAAAAaEGavUnhDyZTBTx//IQAAAbvf8PF/InYofVfkMYL3tOXurZwvzrrPLc7IBmtHGWca39bUlIrleWSiwVIkXgCCfua/aNKZLnFAvkdFG0Jjzvv3VeLeiwwfsplzx0eQ/kvbbEuJLsPItGXAAAANgGe3GpH/wAADy5JF6gnQTxaxTRQvw4cTpwcG1AB+8rXjLdxjHULIEGrHRpW2lYvzjR1dxJLuQAAAGFBmt5J4Q8mUwIR//3hAAADAKxnIwp8mvzVv+/IA9Y2usImlY/51An1EOiAGjnYQLyTO1AxOV2ftF4i37XpywI0S0guqY8Xxzu8QwTAmJ7SRTKrrVPARoltNcn1wBe0ddTMAAAArEGa4knhDyZTAhX//jhAAAAsftbLWdAsl8jtXPeORMrvaV7HCmAFx6QDWS1HAOWSEESs5WS2VURfHaYc0yvcaK7KOwir1P7Nwy+2nXC5xZnfUKg7x9SO4neZ+ZKHFsKWlGjMNtrmOKyyoAkInyBLfhlceBgVWjPxq4ssFL4Z7HOfsXduPKNc12gFLff1bvpKgcdxZSHRyncbZPW+V5pYx90jPEzCHNL61eA2W0AAAABwQZ8ARRE8I/8AAAMDn/0Y9bRiUC4EE04F85eiS7fnB88ow+wgg8auySfUMLGW76dv/vCKNZvST/S9cDFtJt2+LaUVRCuFUSkB224TQWRHa9NkPyDXaogcO1/n129dz9fDVwqe3TMfSy0HjgWoMNpUwQAAAEgBnz90R/8AAAXQUliMroRsJ4msJT495FZa7eTFq2G9SV/NhbZBu3/0t/5mADZwrkzlbDBM4DnjP6h/kbpyv6xZtyN8vb1/eTcAAABvAZ8hakf/AAADAiwgleahVxe8LDELYBwAquhNEynC1cLGl9qt+vq80rDh06rZ59PBK3U9v9D9qfbp8ygQjpsqRLSifrXnWXDB1FJiDUCxZqLFiidk930Gqs424NRUIqKgqW5RArsa7Gb7Dtg6o8iZAAABDkGbJUmoQWiZTAhP//3xAAADAA+XpuD+aTVyUtTklQIKwYYHWjNqjAPny36yS7ME2aaPJRBEGRgkRXN7HhZ9WEC+7BNJ6HRGgogrj4xmF+1Ka4Wxaf1AJZ+vquNPClmrrmW9cxx+WB4Vfoox1V2VZYidey0uJ0atw38ttt1qFifzGAIodd/KogHrC1JRFjUMp7bPzANdJgnUbk5wtMBE6tAZX8/0Hpod/gQLP4chSwmQsLpZPeMQcFTW/pCN2Mvha7/xFU7TY2CRzwNRC2gbOkqivxfOAZbyjW34VmNTmcQhWKT+7NHWpwjsWX3sHthFa+gh9sAWWr/ypa1GE7bFT7wgY+XYVF25fldOGfhJwAAAAGtBn0NFESwj/wAAAwCG7xHVlPGcKWGlmuwD+WRLJIGwAOqeorhIa/IV29sVmjYsK7j3i3RgSBCWZGK+qZstNoVMA7LVHEq+hgwPazczIUD2dUCr8wLPg0GTxjvNKjSyhnBNI1Q5xNswj8jPgQAAAGQBn2RqR/8AAAMA1/tk8b+xhtHg21Arse4J1mKod5BGa1eeIAKrh9sZ13xnv8nABs/v5vXH6WMuEe/5z6wMRkmfy/DiAVIygMKIXJS2jQ4T0I2YtkjwWBOCRcy4oTGycLfrqe7pAAAAvkGbZkmoQWyZTAhP//3xAAADAAX08RPvD6/39HB5/vWG+MACbIt1K+hYdfrunPi64s4NmXZeUpFPK8sNqqv3SmhwKRkpTWwbOZOvbphVneE93qNXDqeqa3EZMtVV/r7l0QWfjTasU0yTXJsWwuyEE2W94bNUMWsipZTfJdag7khKrX+7GQuB/J4cB7YU83hrUWFhl5+GykuTPVkZQKf5w3elamsDTwahRHFJYFUwMYNrqx0aY1bO5zGTOEBHZUEAAACxQZuISeEKUmUwUVLH//yEAAADACTetTpEX5GWnBllmc5QBEgo/c6mH5WXpMpy2lfhaEXwuEHehQy5g0oARS7SmPwpF/xvOf/MmY1OrXIPRG0YpvRuPKcb+oCibHpEVBrPDqMi+ASFbfCuOqn8tGgl0m15gN1hm1bxDc51YYeDtvrEXAKgyIyoCskXYDRjC2JX+GgnOWYKrTgAZkvohpoCzL0ngQlcQwmY2iTgV+wQCXNBAAAAIwGfp2pH/wAAAwBQ4gu4Zs//PX02mfuvq1gxwnY+teog6D5gAAAGJ21vb3YAAABsbXZoZAAAAAAAAAAAAAAAAAAAA+gAAAW0AAEAAAEAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAAVRdHJhawAAAFx0a2hkAAAAAwAAAAAAAAAAAAAAAQAAAAAAAAW0AAAAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAJYAAABkAAAAAAAJGVkdHMAAAAcZWxzdAAAAAAAAAABAAAFtAAAAgAAAQAAAAAEyW1kaWEAAAAgbWRoZAAAAAAAAAAAAAAAAAAAMgAAAEkAVcQAAAAAAC1oZGxyAAAAAAAAAAB2aWRlAAAAAAAAAAAAAAAAVmlkZW9IYW5kbGVyAAAABHRtaW5mAAAAFHZtaGQAAAABAAAAAAAAAAAAAAAkZGluZgAAABxkcmVmAAAAAAAAAAEAAAAMdXJsIAAAAAEAAAQ0c3RibAAAAJhzdHNkAAAAAAAAAAEAAACIYXZjMQAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAJYAZAASAAAAEgAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABj//wAAADJhdmNDAWQAH//hABlnZAAfrNlAmDPl4QAAAwABAAADAGQPGDGWAQAGaOvjyyLAAAAAGHN0dHMAAAAAAAAAAQAAAEkAAAEAAAAAFHN0c3MAAAAAAAAAAQAAAAEAAAIAY3R0cwAAAAAAAAA+AAAAAQAAAgAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAMAAAAAAQAAAQAAAAABAAAEAAAAAAIAAAEAAAAAAQAAAwAAAAABAAABAAAAAAEAAAQAAAAAAgAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAQAAAAAAgAAAQAAAAABAAACAAAAAAEAAAQAAAAAAgAAAQAAAAABAAAEAAAAAAIAAAEAAAAAAQAAAwAAAAABAAABAAAAAAEAAAQAAAAAAgAAAQAAAAABAAADAAAAAAEAAAEAAAAAAQAABAAAAAACAAABAAAAAAEAAAMAAAAAAQAAAQAAAAABAAADAAAAAAEAAAEAAAAAAQAAAwAAAAABAAABAAAAAAEAAAMAAAAAAQAAAQAAAAABAAADAAAAAAEAAAEAAAAAAwAAAgAAAAABAAADAAAAAAEAAAEAAAAAAQAAAwAAAAABAAABAAAAAAIAAAIAAAAAAQAAAwAAAAABAAABAAAAAAEAAAMAAAAAAQAAAQAAAAABAAACAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABAAAAAACAAABAAAAAAEAAAIAAAAAAQAAAwAAAAABAAABAAAAABxzdHNjAAAAAAAAAAEAAAABAAAASQAAAAEAAAE4c3RzegAAAAAAAAAAAAAASQAACVYAAAGeAAAAbQAAAD4AAABUAAABFwAAAGsAAAEZAAAAdAAAAEwAAADVAAAAawAAAOYAAABpAAAAUgAAAMcAAABUAAAAUgAAAE4AAACrAAAAWQAAAE0AAACRAAAAxAAAAFQAAABaAAAAlAAAAGAAAABNAAAAzwAAAGEAAAC2AAAAXwAAAGMAAADQAAAAPwAAAKsAAABIAAAAMQAAAHQAAAA3AAAAjQAAAE0AAAB4AAAAOQAAAGsAAABDAAAAbQAAADsAAAB5AAAAlAAAAFgAAACSAAAAMAAAAIEAAABBAAAAYQAAAHcAAABdAAAAMQAAAGwAAAA6AAAAZQAAALAAAAB0AAAATAAAAHMAAAESAAAAbwAAAGgAAADCAAAAtQAAACcAAAAUc3RjbwAAAAAAAAABAAAAMAAAAGJ1ZHRhAAAAWm1ldGEAAAAAAAAAIWhkbHIAAAAAAAAAAG1kaXJhcHBsAAAAAAAAAAAAAAAALWlsc3QAAAAlqXRvbwAAAB1kYXRhAAAAAQAAAABMYXZmNTguMjkuMTAw' controls>Sorry, seems like your browser doesn't support HTML5 audio/video</video></div>"
      ],
      "text/plain": [
       "<moviepy.video.io.html_tools.HTML2 object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"save_best\": False,\n",
    "    \"base_dir\": \"${gym_env.env_name}/td3-S${algorithm.seed}_${current_time:}\",\n",
    "    \"collect_stats\": True,\n",
    "    # Set to true to have an insight on the learned policy\n",
    "    # (but slows down the evaluation a lot!)\n",
    "    \"plot_agents\": True,\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 1,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"epsilon\": 0.02,\n",
    "        \"n_envs\": 1,\n",
    "        \"n_steps\": 100,\n",
    "        \"nb_evals\": 10,\n",
    "        \"discount_factor\": 0.98,\n",
    "        \"buffer_size\": 2e5,\n",
    "        \"batch_size\": 64,\n",
    "        \"tau_target\": 0.05,\n",
    "        \"eval_interval\": 2_000,\n",
    "        \"max_epochs\": 1_000, #11_000,\n",
    "        # Minimum number of transitions before learning starts\n",
    "        \"learning_starts\": 10000,\n",
    "        \"action_noise\": 0.1,\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": [400, 300],\n",
    "            \"critic_hidden_size\": [400, 300],\n",
    "        },\n",
    "    },    \n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"LunarLanderContinuous-v2\",\n",
    "    },\n",
    "    \"actor_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "    \"critic_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "}\n",
    "\n",
    "td3 = TD3(OmegaConf.create(params))\n",
    "run_td3(td3)\n",
    "td3.visualize_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5b60dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the policy\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
