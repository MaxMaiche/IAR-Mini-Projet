{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ad0e1a3",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[easypip] Installing bbrl_utils\n"
     ]
    }
   ],
   "source": [
    "# Prepare the environment\n",
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "\n",
    "    assert (\n",
    "        run([\"pip\", \"install\", \"easypip\"]).returncode == 0\n",
    "    ), \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils\").setup(maze_mdp=True)\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import bbrl_gymnasium  # noqa: F401\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent\n",
    "from bbrl_utils.algorithms import EpochBasedAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer, soft_update_params\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from bbrl.visu.plot_policies import plot_policy\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1732edc6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ContinuousQAgent(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        self.is_q_function = True\n",
    "        self.model = build_mlp(\n",
    "            [state_dim + action_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        # Get the current state $s_t$ and the chosen action $a_t$\n",
    "        obs = self.get((\"env/env_obs\", t))  # shape B x D_{obs}\n",
    "        action = self.get((\"action\", t))  # shape B x D_{action}\n",
    "\n",
    "        # Compute the Q-value(s_t, a_t)\n",
    "        obs_act = torch.cat((obs, action), dim=1)  # shape B x (D_{obs} + D_{action})\n",
    "        # Get the q-value (and remove the last dimension since it is a scalar)\n",
    "        q_value = self.model(obs_act).squeeze(-1)\n",
    "        self.set((f\"{self.prefix}q_value\", t), q_value)\n",
    "\n",
    "    def predict_value(self, obs, action):\n",
    "        obs_act = torch.cat((obs, action), dim=0)\n",
    "        q_value = self.model(obs_act)\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97fe4ed3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ContinuousDeterministicActor(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        layers = [state_dim] + list(hidden_layers) + [action_dim]\n",
    "        self.model = build_mlp(\n",
    "            layers, activation=nn.ReLU(), output_activation=nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = self.model(obs)\n",
    "        self.set((\"action\", t), action)\n",
    "\n",
    "    def predict_action(self, obs, stochastic):\n",
    "        assert (\n",
    "            not stochastic\n",
    "        ), \"ContinuousDeterministicActor cannot provide stochastic predictions\"\n",
    "        return self.model(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49d88bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b1ada13",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class AddGaussianNoise(Agent):\n",
    "    def __init__(self, sigma):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        act = self.get((\"action\", t))\n",
    "        dist = Normal(act, self.sigma)\n",
    "        action = dist.sample()\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "249794ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddOUNoise(Agent):\n",
    "    \"\"\"\n",
    "    Ornstein Uhlenbeck process noise for actions as suggested by DDPG paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, std_dev, theta=0.15, dt=1e-2):\n",
    "        self.theta = theta\n",
    "        self.std_dev = std_dev\n",
    "        self.dt = dt\n",
    "        self.x_prev = 0\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        act = self.get((\"action\", t))\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (act - self.x_prev) * self.dt\n",
    "            + self.std_dev * math.sqrt(self.dt) * torch.randn(act.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        self.set((\"action\", t), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b2da720",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def compute_actor_loss(q_values):\n",
    "    \"\"\"Returns the actor loss\n",
    "\n",
    "    :param q_values: The q-values (shape 2xB)\n",
    "    :return: A scalar (the loss)\n",
    "    \"\"\"\n",
    "    return -q_values[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "943cade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_critic_loss_td3(cfg, reward, must_bootstrap, q_values, target_q_values_1, target_q_values_2):\n",
    "    \"\"\"\n",
    "    Compute the TD3 critic loss from a sample of transitions\n",
    "    \"\"\"\n",
    "    target = reward[1] + cfg.algorithm.discount_factor * torch.min(target_q_values_1[1], target_q_values_2[1]) * must_bootstrap[1].int()\n",
    "    mse = nn.MSELoss()\n",
    "    critic_loss = mse(q_values[0], target)\n",
    "    \n",
    "    return critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a96a6a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(EpochBasedAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        # Define the agents and optimizers for TD3\n",
    "\n",
    "        # we create the critic and the actor, but also an exploration agent to\n",
    "        # add noise and a target critic. The version below does not use a target\n",
    "        # actor as it proved hard to tune, but such a target actor is used in\n",
    "        # the original paper.\n",
    "\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "        self.critic_1 = ContinuousQAgent(\n",
    "            obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n",
    "        ).with_prefix(\"critic_1/\")\n",
    "        \n",
    "        self.critic_2 = ContinuousQAgent(\n",
    "            obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n",
    "        ).with_prefix(\"critic_2/\")\n",
    "        \n",
    "        self.target_critic_1 = copy.deepcopy(self.critic_1).with_prefix(\"target-critic_1/\")\n",
    "        self.target_critic_2 = copy.deepcopy(self.critic_2).with_prefix(\"target-critic_2/\")\n",
    "\n",
    "    \n",
    "        self.actor = ContinuousDeterministicActor(\n",
    "            obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
    "        )\n",
    "\n",
    "        # As an alternative, you can use `AddOUNoise`\n",
    "        noise_agent = AddGaussianNoise(cfg.algorithm.action_noise)\n",
    "\n",
    "        self.train_policy = Agents(self.actor, noise_agent)\n",
    "        self.eval_policy = self.actor\n",
    "\n",
    "        # Define agents over time\n",
    "        self.t_actor = TemporalAgent(self.actor)\n",
    "        self.t_critic_1 = TemporalAgent(self.critic_1)\n",
    "        self.t_critic_2 = TemporalAgent(self.critic_2)\n",
    "        self.t_target_critic_1 = TemporalAgent(self.target_critic_1)\n",
    "        self.t_target_critic_2 = TemporalAgent(self.target_critic_2)\n",
    "\n",
    "        # Configure the optimizer\n",
    "        self.actor_optimizer = setup_optimizer(cfg.actor_optimizer, self.actor)\n",
    "        self.critic_optimizer_1 = setup_optimizer(cfg.critic_optimizer, self.critic_1)\n",
    "        self.critic_optimizer_2 = setup_optimizer(cfg.critic_optimizer, self.critic_2)\n",
    "\n",
    "\n",
    "def run_td3(td3: TD3):\n",
    "    for rb in td3.iter_replay_buffers():\n",
    "        rb_workspace = rb.get_shuffled(td3.cfg.algorithm.batch_size)\n",
    "\n",
    "        terminated, reward = rb_workspace[\"env/terminated\", \"env/reward\"]\n",
    "\n",
    "        # Determines whether values of the critic should be propagated\n",
    "        # True if the episode reached a time limit or if the task was not done\n",
    "        # See https://github.com/osigaud/bbrl/blob/master/docs/time_limits.md\n",
    "        must_bootstrap = ~terminated\n",
    "\n",
    "        # Random chooser\n",
    "        do1 = False\n",
    "        do2 = False\n",
    "        rdm = torch.randint(0, 3, (1,))\n",
    "        if rdm == 0:\n",
    "            do1 = True\n",
    "            do2 = True\n",
    "        elif rdm == 1:\n",
    "            do1 = True\n",
    "        elif rdm == 2:\n",
    "            do2 = True\n",
    "        \n",
    "        # Critic update\n",
    "        # compute q_values: at t, we have Q(s,a) from the (s,a) in the RB\n",
    "        \n",
    "        td3.t_critic_1(rb_workspace, t=0, n_steps=1)\n",
    "        td3.t_critic_2(rb_workspace, t=0, n_steps=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # replace the action at t+1 in the RB with \\pi(s_{t+1}), to compute\n",
    "            # Q(s_{t+1}, \\pi(s_{t+1}) below\n",
    "            td3.t_actor(rb_workspace, t=1, n_steps=1)\n",
    "            # compute q_values: at t+1 we have Q(s_{t+1}, \\pi(s_{t+1})\n",
    "            td3.t_target_critic_1(rb_workspace, t=1, n_steps=1)\n",
    "            td3.t_target_critic_2(rb_workspace, t=1, n_steps=1)\n",
    "\n",
    "        # finally q_values contains the above collection at t=0 and t=1\n",
    "        q_values_1, post_q_values_1 = rb_workspace[\n",
    "            \"critic_1/q_value\", \"target-critic_1/q_value\"\n",
    "        ]\n",
    "        \n",
    "        q_values_2, post_q_values_2 = rb_workspace[\n",
    "            \"critic_2/q_value\", \"target-critic_2/q_value\"\n",
    "        ]\n",
    "\n",
    "        # Compute critic loss\n",
    "        if do1:\n",
    "            critic_loss_1 = compute_critic_loss_td3(\n",
    "                td3.cfg, reward, must_bootstrap, q_values_1, post_q_values_1, post_q_values_2\n",
    "            )\n",
    "        if do2:\n",
    "            critic_loss_2 = compute_critic_loss_td3(\n",
    "                td3.cfg, reward, must_bootstrap, q_values_2, post_q_values_2, post_q_values_1\n",
    "            )\n",
    "        if do1:\n",
    "            td3.logger.add_log(\"critic_loss_1\", critic_loss_1, td3.nb_steps)\n",
    "        if do2:    \n",
    "            td3.logger.add_log(\"critic_loss_2\", critic_loss_2, td3.nb_steps)\n",
    "        \n",
    "        \n",
    "        if do1:\n",
    "            td3.critic_optimizer_1.zero_grad()\n",
    "            critic_loss_1.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                td3.critic_1.parameters(), td3.cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            td3.critic_optimizer_1.step()\n",
    "        if do2:\n",
    "            td3.critic_optimizer_2.zero_grad()\n",
    "            critic_loss_2.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                td3.critic_2.parameters(), td3.cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            td3.critic_optimizer_2.step()\n",
    "\n",
    "        # Actor update\n",
    "\n",
    "        # Now we determine the actions the current policy would take in the states from the RB\n",
    "        td3.t_actor(rb_workspace, t=0, n_steps=1)\n",
    "\n",
    "        if do1: \n",
    "            # We determine the Q values resulting from actions of the current policy\n",
    "            td3.t_critic_1(rb_workspace, t=0, n_steps=1)\n",
    "        if do2:\n",
    "            td3.t_critic_2(rb_workspace, t=0, n_steps=1)\n",
    "                \n",
    "        \n",
    "\n",
    "        # and we back-propagate the corresponding loss to maximize the Q values\n",
    "        q_values = rb_workspace[\"critic_1/q_value\"]\n",
    "        actor_loss = compute_actor_loss(q_values)\n",
    "\n",
    "        td3.logger.add_log(\"actor_loss\", actor_loss, td3.nb_steps)\n",
    "\n",
    "        # if -25 < actor_loss < 0 and nb_steps > 2e5:\n",
    "        td3.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            td3.actor.parameters(), td3.cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        td3.actor_optimizer.step()\n",
    "\n",
    "        # Soft update of target q function\n",
    "        soft_update_params(\n",
    "            td3.critic_1, td3.target_critic_1, td3.cfg.algorithm.tau_target\n",
    "        )\n",
    "\n",
    "        if td3.evaluate():\n",
    "            if td3.cfg.plot_agents:\n",
    "                plot_policy(\n",
    "                    td3.actor,\n",
    "                    td3.eval_env,\n",
    "                    td3.best_reward,\n",
    "                    str(td3.base_dir / \"plots\"),\n",
    "                    td3.cfg.gym_env.env_name,\n",
    "                    stochastic=False,\n",
    "                )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edab124",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81372d39",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1af05a2e8f4ba2bcfb2cd84f3eac73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 43\u001b[0m\n\u001b[0;32m      1\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_best\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{gym_env.env_name}\u001b[39;00m\u001b[38;5;124m/td3-S$\u001b[39m\u001b[38;5;132;01m{algorithm.seed}\u001b[39;00m\u001b[38;5;124m_$\u001b[39m\u001b[38;5;132;01m{current_time:}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     },\n\u001b[0;32m     40\u001b[0m }\n\u001b[0;32m     42\u001b[0m td3 \u001b[38;5;241m=\u001b[39m TD3(OmegaConf\u001b[38;5;241m.\u001b[39mcreate(params))\n\u001b[1;32m---> 43\u001b[0m \u001b[43mrun_td3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtd3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m td3\u001b[38;5;241m.\u001b[39mvisualize_best()\n",
      "Cell \u001b[1;32mIn[14], line 49\u001b[0m, in \u001b[0;36mrun_td3\u001b[1;34m(td3)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_td3\u001b[39m(td3: TD3):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rb \u001b[38;5;129;01min\u001b[39;00m td3\u001b[38;5;241m.\u001b[39miter_replay_buffers():\n\u001b[0;32m     50\u001b[0m         rb_workspace \u001b[38;5;241m=\u001b[39m rb\u001b[38;5;241m.\u001b[39mget_shuffled(td3\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m     52\u001b[0m         terminated, reward \u001b[38;5;241m=\u001b[39m rb_workspace[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv/terminated\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv/reward\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\bbrl_utils\\algorithms.py:291\u001b[0m, in \u001b[0;36mEpochBasedAlgo.iter_replay_buffers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    289\u001b[0m     train_workspace\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    290\u001b[0m     train_workspace\u001b[38;5;241m.\u001b[39mcopy_n_last_steps(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_workspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstochastic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mn_steps \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mn_envs\n\u001b[0;32m    300\u001b[0m \u001b[38;5;66;03m# Add transitions to buffer\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\bbrl\\agents\\utils.py:79\u001b[0m, in \u001b[0;36mTemporalAgent.__call__\u001b[1;34m(self, workspace, t, n_steps, stop_variable, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m _t \u001b[38;5;241m=\u001b[39m t\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent(workspace, t\u001b[38;5;241m=\u001b[39m_t, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_variable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m         s \u001b[38;5;241m=\u001b[39m workspace\u001b[38;5;241m.\u001b[39mget(stop_variable, _t)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\bbrl\\agents\\utils.py:31\u001b[0m, in \u001b[0;36mAgents.__call__\u001b[1;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, workspace, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents:\n\u001b[1;32m---> 31\u001b[0m         a(workspace, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\bbrl\\agents\\agent.py:84\u001b[0m, in \u001b[0;36mAgent.__call__\u001b[1;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m workspace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Agent.__call__] workspace must not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace \u001b[38;5;241m=\u001b[39m workspace\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\bbrl\\agents\\gymnasium.py:480\u001b[0m, in \u001b[0;36mParallelGymAgent.forward\u001b[1;34m(self, t, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step(k, dict_slice(k, action))\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;66;03m# Use last frame\u001b[39;00m\n\u001b[0;32m    483\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_frame[k]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\bbrl\\agents\\gymnasium.py:433\u001b[0m, in \u001b[0;36mParallelGymAgent._step\u001b[1;34m(self, k, action)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timestep[k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulated_reward[k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m--> 433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_obs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\bbrl\\agents\\gymnasium.py:361\u001b[0m, in \u001b[0;36mParallelGymAgent._format_obs\u001b[1;34m(self, k, obs, info, terminated, truncated, reward)\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    353\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_obs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: value\n\u001b[0;32m    354\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m ParallelGymAgent\u001b[38;5;241m.\u001b[39m_flatten_value(observation)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    355\u001b[0m         }\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservation must be a torch.Tensor or a dict, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(observation)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m     )\n\u001b[1;32m--> 361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_obs\u001b[39m(\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m, k: \u001b[38;5;28mint\u001b[39m, obs, info, \u001b[38;5;241m*\u001b[39m, terminated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, truncated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    363\u001b[0m ):\n\u001b[0;32m    364\u001b[0m     observation: Union[Tensor, Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]] \u001b[38;5;241m=\u001b[39m ParallelGymAgent\u001b[38;5;241m.\u001b[39m_format_frame(\n\u001b[0;32m    365\u001b[0m         obs\n\u001b[0;32m    366\u001b[0m     )\n\u001b[0;32m    368\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"save_best\": False,\n",
    "    \"base_dir\": \"${gym_env.env_name}/td3-S${algorithm.seed}_${current_time:}\",\n",
    "    \"collect_stats\": True,\n",
    "    # Set to true to have an insight on the learned policy\n",
    "    # (but slows down the evaluation a lot!)\n",
    "    \"plot_agents\": True,\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 1,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"epsilon\": 0.02,\n",
    "        \"n_envs\": 1, # 1,\n",
    "        \"n_steps\": 100,\n",
    "        \"nb_evals\": 10,\n",
    "        \"discount_factor\": 0.99,\n",
    "        \"buffer_size\": 2e5, #2e5,\n",
    "        \"batch_size\": 64,\n",
    "        \"tau_target\": 0.05,\n",
    "        \"eval_interval\": 2_000,\n",
    "        \"max_epochs\": 20_000,\n",
    "        # Minimum number of transitions before learning starts\n",
    "        \"learning_starts\": 10000,\n",
    "        \"action_noise\": 0.1, #0.1,\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": [500,400],#[400, 300],\n",
    "            \"critic_hidden_size\": [500, 500], # [400, 300],\n",
    "        },\n",
    "    },    \n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"LunarLanderContinuous-v2\",\n",
    "    },\n",
    "    \"actor_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "    \"critic_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "}\n",
    "\n",
    "td3 = TD3(OmegaConf.create(params))\n",
    "run_td3(td3)\n",
    "td3.visualize_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5a515c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838588e81b3540d7a0bf110148403b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d463dcb012ce49fa816b5292bb25a958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1e2a1fa737480189b354df07d2bfc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082d9446873d43ec9fa1f690fc435eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec17f906194a49d4a759e99ed9264266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9f9bd895434a938e44997ad435f8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45b87635db54453bbc69c821c83bf22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c675275447b47b5955b905c052da811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04c92e4c1e74c00ba3bb59978c73893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcfee61b9ca347b580180531892e2198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Do one 10 seed run\n",
    "seeds = [np.random.randint(0, 1000) for _ in range(10)]\n",
    "td3s = []\n",
    "params['base_dir'] += \"_10_runs_10k_epochs\"\n",
    "params['algorithm']['max_epochs'] = 10_000\n",
    "for seed in seeds:\n",
    "    params[\"algorithm\"][\"seed\"] = seed\n",
    "    td3 = TD3(OmegaConf.create(params))\n",
    "    run_td3(td3)\n",
    "    td3s.append(td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b6215ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video of best agent recorded in outputs\\LunarLanderContinuous-v2\\td3-S114_20241008-152845_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "Moviepy - Building video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S114_20241008-152845_10_runs_2_10_runs_10k_epochs\\best_agent.mp4.\n",
      "Moviepy - Writing video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S114_20241008-152845_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S114_20241008-152845_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "Video of best agent recorded in outputs\\LunarLanderContinuous-v2\\td3-S501_20241008-155623_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "Moviepy - Building video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S501_20241008-155623_10_runs_2_10_runs_10k_epochs\\best_agent.mp4.\n",
      "Moviepy - Writing video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S501_20241008-155623_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S501_20241008-155623_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "Video of best agent recorded in outputs\\LunarLanderContinuous-v2\\td3-S929_20241008-162641_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "Moviepy - Building video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S929_20241008-162641_10_runs_2_10_runs_10k_epochs\\best_agent.mp4.\n",
      "Moviepy - Writing video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S929_20241008-162641_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S929_20241008-162641_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "Video of best agent recorded in outputs\\LunarLanderContinuous-v2\\td3-S245_20241008-165500_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "Moviepy - Building video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S245_20241008-165500_10_runs_2_10_runs_10k_epochs\\best_agent.mp4.\n",
      "Moviepy - Writing video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S245_20241008-165500_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S245_20241008-165500_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "Video of best agent recorded in outputs\\LunarLanderContinuous-v2\\td3-S309_20241008-172151_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "Moviepy - Building video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S309_20241008-172151_10_runs_2_10_runs_10k_epochs\\best_agent.mp4.\n",
      "Moviepy - Writing video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S309_20241008-172151_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S309_20241008-172151_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "Video of best agent recorded in outputs\\LunarLanderContinuous-v2\\td3-S339_20241008-174847_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "Moviepy - Building video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S339_20241008-174847_10_runs_2_10_runs_10k_epochs\\best_agent.mp4.\n",
      "Moviepy - Writing video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S339_20241008-174847_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S339_20241008-174847_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "Video of best agent recorded in outputs\\LunarLanderContinuous-v2\\td3-S418_20241008-181324_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "Moviepy - Building video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S418_20241008-181324_10_runs_2_10_runs_10k_epochs\\best_agent.mp4.\n",
      "Moviepy - Writing video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S418_20241008-181324_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S418_20241008-181324_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "Video of best agent recorded in outputs\\LunarLanderContinuous-v2\\td3-S987_20241008-184110_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "Moviepy - Building video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S987_20241008-184110_10_runs_2_10_runs_10k_epochs\\best_agent.mp4.\n",
      "Moviepy - Writing video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S987_20241008-184110_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S987_20241008-184110_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "Video of best agent recorded in outputs\\LunarLanderContinuous-v2\\td3-S55_20241008-190557_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "Moviepy - Building video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S55_20241008-190557_10_runs_2_10_runs_10k_epochs\\best_agent.mp4.\n",
      "Moviepy - Writing video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S55_20241008-190557_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S55_20241008-190557_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "Video of best agent recorded in outputs\\LunarLanderContinuous-v2\\td3-S113_20241008-193624_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "Moviepy - Building video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S113_20241008-193624_10_runs_2_10_runs_10k_epochs\\best_agent.mp4.\n",
      "Moviepy - Writing video C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S113_20241008-193624_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\Max\\Desktop\\code\\M2\\IAR\\IAR-Mini-Projet\\outputs\\LunarLanderContinuous-v2\\td3-S113_20241008-193624_10_runs_2_10_runs_10k_epochs\\best_agent.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Visualize the best agent\n",
    "for td3 in td3s:\n",
    "    td3.visualize_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5b60dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset tensorboard\n",
    "#!kill 23440\n",
    "\n",
    "# Start tensorboard\n",
    "#!tensorboard --logdir outputs/tblogs --port 6006 --bind_all\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
